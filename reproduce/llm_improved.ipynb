{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install transformers accelerate torch sentencepiece bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (if needed for gated models)\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# I/O paths - adjust these for your environment\n",
    "queries_path = \"/content/data/query_data.jsonl\"\n",
    "os.makedirs(\"/content/outputs\", exist_ok=True)\n",
    "model_stub = model_name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "out_path = f\"/content/outputs/{model_stub}_preds.jsonl\"\n",
    "\n",
    "# Improved generation config\n",
    "gen_cfg = dict(\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "# Batching config for speed\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Quantization config for memory efficiency and speed\n",
    "USE_QUANTIZATION = True\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPROVED LLM INFERENCE CODE - v2 (with per-example slicing fix)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using device={device}, dtype={torch_dtype}, writing to {out_path}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Quantization: {USE_QUANTIZATION}\")\n",
    "print(f\"Generation config: do_sample={gen_cfg[\\'do_sample\\']}, repetition_penalty={gen_cfg[\\'repetition_penalty\\']}\")\n",
    "print(f\"Using chat template: True\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with optional quantization\n",
    "if USE_QUANTIZATION and torch.cuda.is_available():\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch_dtype,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"sdpa\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model & tokenizer loaded.\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries\n",
    "queries = [json.loads(l) for l in open(queries_path, \"r\", encoding=\"utf-8\")]\n",
    "print(f\"Loaded {len(queries)} prompts from {queries_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_with_chat_template(context, anchor):\n",
    "    \"\"\"\n",
    "    Prepare prompt using the model\\'s chat template for better instruction following.\n",
    "    \"\"\"\n",
    "    user_message = f\\'{context}\\\\nHere is a sentence: {anchor}.\\\\nNow copy and output all the text before \"Here is a sentence\". Do not change any words.\\'\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_batch(batch_queries):\n",
    "    \"\"\"\n",
    "    Generate outputs for a batch of queries.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    contexts = []\n",
    "    for ex in batch_queries:\n",
    "        anchor = ex.get(\"anchor\") or \"\"\n",
    "        context = ex.get(\"context\") or \"\"\n",
    "        prompt = prepare_prompt_with_chat_template(context, anchor)\n",
    "        prompts.append(prompt)\n",
    "        contexts.append(context)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    max_context_tokens = max(len(tokenizer(ctx, add_special_tokens=False).input_ids) for ctx in contexts)\n",
    "    adaptive_max_new_tokens = min(max_context_tokens + 16, gen_cfg[\"max_new_tokens\"])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=adaptive_max_new_tokens,\n",
    "            do_sample=gen_cfg[\"do_sample\"],\n",
    "            temperature=gen_cfg[\"temperature\"] if gen_cfg[\"do_sample\"] else None,\n",
    "            repetition_penalty=gen_cfg[\"repetition_penalty\"],\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    input_lens = inputs[\"attention_mask\"].sum(dim=1).tolist()\n",
    "    decoded_outputs = []\n",
    "    for i, seq in enumerate(outputs):\n",
    "        cont = seq[int(input_lens[i]):]\n",
    "        text = tokenizer.decode(cont, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        decoded_outputs.append(text)\n",
    "    \n",
    "    return decoded_outputs, prompts, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34abeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in batches\n",
    "written = 0\n",
    "empty_outputs = 0\n",
    "errors = 0\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for batch_start in tqdm(range(0, len(queries), BATCH_SIZE), desc=f\"Generating with {model_stub}\"):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(queries))\n",
    "        batch_queries = queries[batch_start:batch_end]\n",
    "        \n",
    "        try:\n",
    "            decoded_outputs, prompts, contexts = generate_batch(batch_queries)\n",
    "            \n",
    "            for i, (ex, output, prompt, context) in enumerate(zip(batch_queries, decoded_outputs, prompts, contexts)):\n",
    "                anchor = ex.get(\"anchor\") or \"\"\n",
    "                \n",
    "                if not output.strip():\n",
    "                    empty_outputs += 1\n",
    "                \n",
    "                n_ctx_tokens = len(tokenizer(context, add_special_tokens=False).input_ids)\n",
    "                \n",
    "                rec = {\n",
    "                    \"model\": model_name,\n",
    "                    \"anchor\": anchor,\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"output\": output or \"\",\n",
    "                    \"use_chat_template\": True,\n",
    "                    \"do_sample\": gen_cfg[\"do_sample\"],\n",
    "                    \"repetition_penalty\": gen_cfg[\"repetition_penalty\"],\n",
    "                    \"max_new_tokens\": gen_cfg[\"max_new_tokens\"],\n",
    "                    \"batch_size\": BATCH_SIZE,\n",
    "                    \"context_tokens\": n_ctx_tokens,\n",
    "                }\n",
    "                \n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\\\n\")\n",
    "                written += 1\n",
    "            \n",
    "            fout.flush()\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(f\"Error processing batch {batch_start}-{batch_end}: {repr(e)}\")\n",
    "            \n",
    "            for ex in batch_queries:\n",
    "                anchor = ex.get(\"anchor\") or \"\"\n",
    "                context = ex.get(\"context\") or \"\"\n",
    "                prompt = prepare_prompt_with_chat_template(context, anchor)\n",
    "                \n",
    "                rec = {\n",
    "                    \"model\": model_name,\n",
    "                    \"anchor\": anchor,\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"output\": \"\",\n",
    "                    \"error\": repr(e),\n",
    "                }\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\\\n\")\n",
    "                written += 1\n",
    "            \n",
    "            fout.flush()\n",
    "\n",
    "print(f\"Saved {written} generations to {out_path} | empty_outputs={empty_outputs} | errors={errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45db85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"Sample outputs:\")\n",
    "print(\"=\"*80)\n",
    "shown = 0\n",
    "for line in open(out_path, \"r\", encoding=\"utf-8\"):\n",
    "    obj = json.loads(line)\n",
    "    print(f\"\\\\nExample {shown + 1}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Output (first 500 chars):\\\\n{obj[\\'output\\'][:500]}\")\n",
    "    shown += 1\n",
    "    if shown >= 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
