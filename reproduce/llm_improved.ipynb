{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07797830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install transformers accelerate torch sentencepiece bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (if needed for gated models)\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# I/O paths - adjust these for your environment\n",
    "queries_path = \"/content/data/query_data.jsonl\"\n",
    "os.makedirs(\"/content/outputs\", exist_ok=True)\n",
    "model_stub = model_name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "out_path = f\"/content/outputs/{model_stub}_preds.jsonl\"\n",
    "\n",
    "gen_cfg = dict(\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    top_k=60,\n",
    "    top_p=0.9,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.8,\n",
    ")\n",
    "\n",
    "# Batching config for speed\n",
    "BATCH_SIZE = 8  # Adjust based on available VRAM (8-32 recommended)\n",
    "\n",
    "USE_QUANTIZATION = True  \n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device={device}, dtype={torch_dtype}, writing to {out_path}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Quantization: {USE_QUANTIZATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with optional quantization\n",
    "if USE_QUANTIZATION and torch.cuda.is_available():\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch_dtype,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"sdpa\",  # Use SDPA for attention\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model & tokenizer loaded.\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries\n",
    "queries = [json.loads(l) for l in open(queries_path, \"r\", encoding=\"utf-8\")]\n",
    "print(f\"Loaded {len(queries)} prompts from {queries_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_with_chat_template(context, anchor):\n",
    "    \"\"\"\n",
    "    Prepare prompt using the model's chat template for better instruction following.\n",
    "    \"\"\"\n",
    "    user_message = f'{context}\\nHere is a sentence: {anchor}.\\nNow copy and output all the text before \"Here is a sentence\". Do not change any words.'\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_batch(batch_queries):\n",
    "    \"\"\"\n",
    "    Generate outputs for a batch of queries.\n",
    "    \"\"\"\n",
    "    # Prepare prompts with chat template\n",
    "    prompts = []\n",
    "    for ex in batch_queries:\n",
    "        anchor = ex.get(\"anchor\") or \"\"\n",
    "        context = ex.get(\"context\") or \"\"\n",
    "        prompt = prepare_prompt_with_chat_template(context, anchor)\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # Tokenize with padding and truncation\n",
    "    inputs = tokenizer(\n",
    "        prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=2048  # Adjust based on model's context window\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=gen_cfg[\"max_new_tokens\"],\n",
    "            do_sample=gen_cfg[\"do_sample\"],\n",
    "            temperature=gen_cfg[\"temperature\"] if gen_cfg[\"do_sample\"] else None,\n",
    "            repetition_penalty=gen_cfg[\"repetition_penalty\"],\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (skip input)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[:, input_len:]\n",
    "    \n",
    "    # Batch decode with proper spacing\n",
    "    decoded_outputs = tokenizer.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True  # Helps with punctuation spacing\n",
    "    )\n",
    "    \n",
    "    return decoded_outputs, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae785d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in batches\n",
    "written = 0\n",
    "empty_outputs = 0\n",
    "errors = 0\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    # Process in batches\n",
    "    for batch_start in tqdm(range(0, len(queries), BATCH_SIZE), desc=f\"Generating with {model_stub}\"):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(queries))\n",
    "        batch_queries = queries[batch_start:batch_end]\n",
    "        \n",
    "        try:\n",
    "            decoded_outputs, prompts = generate_batch(batch_queries)\n",
    "            \n",
    "            # Write results\n",
    "            for i, (ex, output, prompt) in enumerate(zip(batch_queries, decoded_outputs, prompts)):\n",
    "                anchor = ex.get(\"anchor\") or \"\"\n",
    "                context = ex.get(\"context\") or \"\"\n",
    "                \n",
    "                if not output.strip():\n",
    "                    empty_outputs += 1\n",
    "                \n",
    "                rec = {\n",
    "                    \"model\": model_name,\n",
    "                    \"anchor\": anchor,\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"output\": output or \"\",\n",
    "                }\n",
    "                \n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                written += 1\n",
    "            \n",
    "            # Flush every batch instead of every line for better performance\n",
    "            fout.flush()\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            print(f\"Error processing batch {batch_start}-{batch_end}: {repr(e)}\")\n",
    "            \n",
    "            # Write error records for this batch\n",
    "            for ex in batch_queries:\n",
    "                anchor = ex.get(\"anchor\") or \"\"\n",
    "                context = ex.get(\"context\") or \"\"\n",
    "                prompt = prepare_prompt_with_chat_template(context, anchor)\n",
    "                \n",
    "                rec = {\n",
    "                    \"model\": model_name,\n",
    "                    \"anchor\": anchor,\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"output\": \"\",\n",
    "                    \"error\": repr(e),\n",
    "                }\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                written += 1\n",
    "            \n",
    "            fout.flush()\n",
    "\n",
    "print(f\"Saved {written} generations to {out_path} | empty_outputs={empty_outputs} | errors={errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample outputs:\")\n",
    "print(\"=\"*80)\n",
    "shown = 0\n",
    "for line in open(out_path, \"r\", encoding=\"utf-8\"):\n",
    "    obj = json.loads(line)\n",
    "    print(f\"\\nExample {shown + 1}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Output (first 500 chars):\\n{obj['output'][:500]}\")\n",
    "    shown += 1\n",
    "    if shown >= 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
