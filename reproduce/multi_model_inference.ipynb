{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4ede17",
   "metadata": {},
   "source": [
    "# Multi-Model Inference for Paper Replication\n",
    "\n",
    "This notebook runs inference across all 7 models from the paper using Together AI API.\n",
    "\n",
    "**Models:**\n",
    "1. Llama2-Chat-7b\n",
    "2. Mistral-Instruct-7b\n",
    "3. SOLAR-10.7b\n",
    "4. Llama2-Chat-13b\n",
    "5. Vicuna-13b\n",
    "6. Mixtral-Instruct-8x7b\n",
    "7. WizardLM-13b\n",
    "\n",
    "**Setup Required:**\n",
    "1. Get Together AI API key from https://api.together.xyz/\n",
    "2. Upload your `query_data.jsonl` to `/content/data/`\n",
    "3. Set your API key in the cell below\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4da0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers accelerate torch sentencepiece bitsandbytes aiohttp requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e772c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Together AI API key here\n",
    "import os\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"your-api-key-here\"  # Replace with your actual API key\n",
    "\n",
    "# Verify it's set\n",
    "if os.environ.get(\"TOGETHER_API_KEY\") == \"your-api-key-here\":\n",
    "    print(\"⚠️  WARNING: Please set your actual Together AI API key above!\")\n",
    "else:\n",
    "    print(\"✓ API key is set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the multi-model inference files\n",
    "# If running from cloned repo, skip this. Otherwise, upload the files.\n",
    "\n",
    "# For now, let's check if files exist\n",
    "import os\n",
    "files_needed = [\"model_registry.py\", \"together_client.py\", \"run_multi_model.py\"]\n",
    "missing = [f for f in files_needed if not os.path.exists(f)]\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing files: {missing}\")\n",
    "    print(\"Please upload these files from the reproduce/ folder\")\n",
    "else:\n",
    "    print(\"✓ All required files present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data file exists\n",
    "import os\n",
    "data_path = \"/content/data/query_data.jsonl\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"⚠️  Data file not found at {data_path}\")\n",
    "    print(\"Please upload your query_data.jsonl to /content/data/\")\n",
    "    \n",
    "    # Create directory\n",
    "    os.makedirs(\"/content/data\", exist_ok=True)\n",
    "else:\n",
    "    # Count queries\n",
    "    with open(data_path) as f:\n",
    "        n_queries = sum(1 for _ in f)\n",
    "    print(f\"✓ Found {n_queries} queries in {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786627d7",
   "metadata": {},
   "source": [
    "## Option 1: Run All Models with API (Recommended)\n",
    "\n",
    "This will run all 7 models using Together AI API. Fast and reliable.\n",
    "\n",
    "**Estimated time:** 15-30 minutes  \n",
    "**Estimated cost:** $0.10-0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models with API\n",
    "!python run_multi_model.py --models all --backend api --queries-path /content/data/query_data.jsonl --output-dir /content/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cf9a5",
   "metadata": {},
   "source": [
    "## Option 2: Run Specific Models\n",
    "\n",
    "Run only the models you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "!python run_multi_model.py --list-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5aefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run specific models (example: just Mistral and Llama2-7B)\n",
    "!python run_multi_model.py --models mistral-7b-instruct llama2-7b-chat --backend api --queries-path /content/data/query_data.jsonl --output-dir /content/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd01dd",
   "metadata": {},
   "source": [
    "## Option 3: Hybrid Approach\n",
    "\n",
    "Run small models locally (free) and large models via API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-select backend based on model size\n",
    "!python run_multi_model.py --models all --backend auto --queries-path /content/data/query_data.jsonl --output-dir /content/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49953f",
   "metadata": {},
   "source": [
    "## Check Results\n",
    "\n",
    "View the generated outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "!ls -lh /content/outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f263d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample outputs from a model\n",
    "import json\n",
    "\n",
    "model_key = \"mistral-7b-instruct\"  # Change this to view other models\n",
    "output_file = f\"/content/outputs/{model_key}_preds.jsonl\"\n",
    "\n",
    "print(f\"Sample outputs from {model_key}:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with open(output_file, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:  # Show first 3 examples\n",
    "            break\n",
    "        obj = json.loads(line)\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Anchor: {obj[\\'anchor\\'][:100]}...\")\n",
    "        print(f\"Output (first 300 chars): {obj[\\'output\\'][:300]}...\")\n",
    "        print(f\"Backend: {obj[\\'backend\\']}\")\n",
    "        print(f\"Use chat template: {obj[\\'use_chat_template\\']}\")\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15945d84",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Download all output files to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a93dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all outputs for easy download\n",
    "!zip -r /content/all_model_outputs.zip /content/outputs/\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"/content/all_model_outputs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6bb7c",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**\"TOGETHER_API_KEY not found\"**  \n",
    "Make sure you set your API key in the cell above and ran that cell.\n",
    "\n",
    "**\"Model not found\" or License Issues**  \n",
    "Some models require accepting licenses on HuggingFace. For Llama2-based models:\n",
    "1. Go to https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "2. Accept the license\n",
    "3. Login: `!huggingface-cli login`\n",
    "\n",
    "**\"CUDA out of memory\"**  \n",
    "If running locally, reduce batch size or use API instead.\n",
    "\n",
    "**Spacing issues in output**  \n",
    "The code uses v2 with the per-example slicing fix. Check that outputs have `\"use_chat_template\": true`.\n",
    "\n",
    "For more help, see `MULTI_MODEL_GUIDE.md` in the repository."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
